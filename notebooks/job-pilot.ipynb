{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import asyncio\n",
    "import assemblyai as aai\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from queue import Queue\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.llms.together import TogetherLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Context,\n",
    "    step,\n",
    "    Workflow,\n",
    "    Event,\n",
    "    InputRequiredEvent,\n",
    "    HumanResponseEvent\n",
    ")\n",
    "from llama_index.readers.whisper import WhisperReader\n",
    "from llama_index.utils.workflow import draw_all_possible_flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voice Feedback Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscriptionHandler:\n",
    "    '''A class to handle transcription of audio files using AssemblyAI API.'''\n",
    "\n",
    "    aai.settings.api_key = os.getenv('ASSEMBLY_API_KEY')\n",
    "    transcriber = aai.Transcriber()\n",
    "\n",
    "    def __init__(self):\n",
    "        self.transcription_queue = Queue()\n",
    "        self.interface = None \n",
    "    \n",
    "    def transcribe_speech(self, file_path):\n",
    "        '''Transcribe the speech in the given audio file.'''\n",
    "        try:\n",
    "            # Create a transcriber object.\n",
    "            transcript = self.transcriber.transcribe(file_path)\n",
    "            return transcript.text\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error during transcription: {str(e)}\")\n",
    "    \n",
    "    def store_transcription(self, transcription):\n",
    "        '''Store the transcription in a queue'''\n",
    "        self.transcription_queue.append(transcription)\n",
    "        return transcription\n",
    "    \n",
    "    def create_interface(self):\n",
    "        '''Create a user interface for the transcription process.'''\n",
    "        mic_transcribe = gr.Interface(\n",
    "            fn=lambda x: self.store_transcription(self.transcribe_speech(x)),\n",
    "            inputs=gr.Audio(sources=\"microphone\", type=\"filepath\"),\n",
    "            outputs=gr.Textbox(label=\"Transcription\")\n",
    "        )\n",
    "        self.interface = gr.Blocks()\n",
    "        with self.interface:\n",
    "            gr.TabbedInterface(\n",
    "                [mic_transcribe],\n",
    "                [\"Transcribe Microphone\"]\n",
    "            )\n",
    "        return self.interface\n",
    "\n",
    "    async def get_transcription(self):\n",
    "        self.interface = self.create_interface()\n",
    "        self.interface.launch(\n",
    "            share=False,\n",
    "            server_port=8000, \n",
    "            prevent_thread_lock=True\n",
    "        )\n",
    "\n",
    "        # poll every 1.5 seconds waiting for something to end up in the queue\n",
    "        attempt = 0\n",
    "        while True:\n",
    "            if not self.transcription_queue.empty():\n",
    "                result = self.transcription_queue.get()\n",
    "                if self.interface is not None:\n",
    "                    self.interface.close()\n",
    "                return result\n",
    "            else:\n",
    "                attempt += 1\n",
    "            if attempt > 10:\n",
    "                if self.interface is not None:\n",
    "                    self.interface.close()\n",
    "                print(\"No transcription received after 10 attempts. Exiting...\")\n",
    "                break\n",
    "            await asyncio.sleep(1.5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseFormEvent(Event):\n",
    "    application_form: str \n",
    "\n",
    "class GenerateQuestionEvent(Event):\n",
    "    pass\n",
    "\n",
    "class QueryEvent(Event):\n",
    "    field: str \n",
    "    query: str\n",
    "\n",
    "class ResponseEvent(Event):\n",
    "    field: str \n",
    "    response: str\n",
    "\n",
    "class FeedbackEvent(Event):\n",
    "    feedback: str "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobPilotWorkflow(Workflow):\n",
    "\n",
    "    storage_dir = '../storage'\n",
    "    llm: TogetherLLM\n",
    "    query_engine: VectorStoreIndex\n",
    "    \n",
    "    @step \n",
    "    async def setup(self, ctx: Context, ev: StartEvent) -> ParseFormEvent:\n",
    "\n",
    "        if not ev.resume:\n",
    "            raise ValueError(\"No resume file provided.\") \n",
    "        if not ev.application_form:\n",
    "            raise ValueError(\"No application form provided\")\n",
    "        \n",
    "        self.llm = TogetherLLM(model=\"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\")\n",
    "        if os.path.exists(self.storage_dir):\n",
    "            # read parsed resume from disk\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=self.storage_dir)\n",
    "            index = load_index_from_storage(\n",
    "                storage_context,\n",
    "                embed_model=HuggingFaceEmbedding(\n",
    "                    model_name=\"BAAI/bge-base-en-v1.5\"\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            # parse and load resume\n",
    "            documents = LlamaParse(\n",
    "                api_key=os.getenv('LLAMA_CLOUDE_API_KEY'),\n",
    "                base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "                result_type=\"markdown\",\n",
    "                system_prompt=\"This is a resume/cv, gather related facts together and format it as bullet points with headers\"\n",
    "            ).load_data(ev.resume)\n",
    "            # embed and index resume\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                documents,\n",
    "                embed_model=HuggingFaceEmbedding(\n",
    "                    model_name=\"BAAI/bge-base-en-v1.5\"\n",
    "                )\n",
    "            )\n",
    "            # store embed and indexed doc to disk\n",
    "            index.storage_context.persist(persist_dir=self.storage_dir)\n",
    "        \n",
    "        # create a query engine\n",
    "        self.query_engine = index.as_query_engine(llm=self.llm, similarity_top_k=5)\n",
    "        return ParseFormEvent(application_form=ev.application_form)\n",
    "    \n",
    "    @step \n",
    "    async def parse_form(self, ctx: Context, ev: ParseFormEvent) -> GenerateQuestionEvent:\n",
    "        # parse and load application form\n",
    "        documents = LlamaParse(\n",
    "            api_key=os.getenv('LLAMA_CLOUDE_API_KEY'),\n",
    "            base_url=os.getenv(\"LLAMA_CLOUD_BASE_URL\"),\n",
    "            result_type=\"markdown\",\n",
    "            system_prompt=\"This is a job application form. Create a list of all the fields that need to be filled in.\",\n",
    "            system_prompt_append=\"Return a bulleted list of the fields ONLY.\"\n",
    "        ).load_data(ev.application_form)\n",
    "        result = documents[0]\n",
    "        raw_json = self.llm.complete(\n",
    "            f\"This is a parsed form. Convert it into a JSON object containing only the list of fields to be filled in, in the form {{ fields: [...] }}. <form>{result.text}</form>. Return JSON ONLY, no markdown.\")\n",
    "        fields = json.loads(raw_json.text)[\"fields\"]\n",
    "\n",
    "        # set list of fields to fillout\n",
    "        await ctx.set('fields', fields)\n",
    "\n",
    "        return GenerateQuestionEvent()\n",
    "\n",
    "    @step \n",
    "    async def generate_questions(self, ctx: Context, ev: GenerateQuestionEvent | FeedbackEvent) -> QueryEvent:\n",
    "\n",
    "        # get list of fields to fillout\n",
    "        fields = await ctx.get('fields')\n",
    "\n",
    "        for field in fields:\n",
    "            question = f\"How would you answer this question about the candidate? <field>{field}</field>\"\n",
    "\n",
    "            # if incomming even is feedback\n",
    "            if hasattr(ev,\"feedback\"):\n",
    "                question += f\"\"\"\n",
    "                    \\nWe previously got feedback about how we answered the questions.\n",
    "                    It might not be relevant to this particular field, but here it is:\n",
    "                    <feedback>{ev.feedback}</feedback>\n",
    "                \"\"\"\n",
    "\n",
    "            ctx.send_event(\n",
    "                QueryEvent(\n",
    "                    field=field,\n",
    "                    query=question\n",
    "                )\n",
    "            )\n",
    "         # store number of total fields\n",
    "        await ctx.set('total_fields', len(fields))\n",
    "\n",
    "        return\n",
    "\n",
    "    @step \n",
    "    async def ask_question(self, cxt: Context, ev: QueryEvent) -> ResponseEvent:\n",
    "        \n",
    "        response = self.query_engine.query(f\"This is a question about the specific resume/cv we have in our database: {ev.query}\")\n",
    "        return ResponseEvent(field=ev.field, response=response.response)\n",
    "    \n",
    "    @step \n",
    "    async def fill_application(self, ctx: Context, ev: ResponseEvent) -> InputRequiredEvent | StopEvent:\n",
    "        \n",
    "        total_fields = await ctx.get('total_fields')\n",
    "        responses = ctx.collect_events(ev, [ResponseEvent] * total_fields)\n",
    "        if not responses:\n",
    "            return None \n",
    "        \n",
    "        # all responses list\n",
    "        responses_list = '\\n'.join(f'Field: {r.field} \\nResponse: {r.response}' for r in responses)\n",
    "        result = self.llm.complete(f\"\"\"\n",
    "            You are given a list of fields in an application form and responses to\n",
    "            questions about those fields from a resume. Combine the two into a list of\n",
    "            fields and succinct, factual answers to fill in those fields.\n",
    "\n",
    "            <responses>\n",
    "            {responses_list}\n",
    "            </responses>\n",
    "        \"\"\")\n",
    "\n",
    "        # save the result form\n",
    "        await ctx.set('filled_form', str(result))\n",
    "\n",
    "        # let's get a human in the loop\n",
    "        return InputRequiredEvent(\n",
    "            prefix=\"How does this look? Give me any feedback you have on any of the answers.\",\n",
    "            result=result\n",
    "        ) \n",
    "    \n",
    "    @step \n",
    "    async def get_feedback(self, ctx: Context, ev: HumanResponseEvent) -> FeedbackEvent | StopEvent:\n",
    "        \n",
    "        result = self.llm.complete(f\"\"\"\n",
    "            You have received some human feedback on the form-filling task you've done.\n",
    "            Does everything look good, or is there more work to be done?\n",
    "            <feedback>\n",
    "            {ev.response}\n",
    "            </feedback>\n",
    "            If everything is fine, respond with just the word 'OKAY'.\n",
    "            If there's any other feedback, respond with just the word 'FEEDBACK'.\n",
    "        \"\"\")\n",
    "\n",
    "        verdict = result.text.strip()\n",
    "\n",
    "        print(f\"LLM says the verdict was {verdict}\")\n",
    "        if (verdict == \"OKAY\"):\n",
    "            return StopEvent(result=await ctx.get(\"filled_form\"))\n",
    "        else:\n",
    "            return FeedbackEvent(feedback=ev.response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 5a1bb358-8a16-475b-ba5e-cdc831fee2eb\n",
      "We've filled in your form! Here are the results:\n",
      "\n",
      "Here is the list of fields with succinct, factual answers:\n",
      "\n",
      "1. **First Name**: Sarah\n",
      "2. **Last Name**: Chen\n",
      "3. **Email**: sarah.chen@email.com\n",
      "4. **Phone**: Not listed\n",
      "5. **Linkedin**: linkedin.com/in/sarahchen\n",
      "6. **Project Portfolio**: The candidate has showcased two significant projects: EcoTrack (a full-stack carbon footprint tracking application) and ChatFlow (a real-time chat application with end-to-end encryption).\n",
      "7. **Degree**: Bachelor of Science in Computer Science with a minor in User Experience Design from the University of California, Berkeley\n",
      "8. **Graduation Date**: 2017\n",
      "9. **Current Job Title**: Senior Full Stack Developer\n",
      "10. **Current Employer**: TechFlow Solutions\n",
      "11. **Technical Skills**: Proficient in React.js, Redux, Next.js, TypeScript, Node.js, Express.js, GraphQL, PostgreSQL, MongoDB, Docker, Kubernetes, AWS, and more.\n",
      "12. **Describe why youâ€™re a good fit for this position**: The candidate is a highly skilled Full Stack Web Developer with 6+ years of experience, expertise in React, Node.js, and cloud architecture, and a proven track record of leading technical teams and implementing CI/CD pipelines.\n",
      "13. **Do you have 5 years of experience in React?**: No\n",
      "\n",
      "Let me know if you'd like me to make any adjustments.\n",
      "* Running on local URL:  http://127.0.0.1:8000\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:8000/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jobpilot = JobPilotWorkflow(timeout=600, verbose=False)\n",
    "handler = jobpilot.run(\n",
    "    resume='../data/fake_resume.pdf',\n",
    "    application_form='../data/fake_application_form.pdf'\n",
    ")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, InputRequiredEvent):\n",
    "        print(\"We've filled in your form! Here are the results:\\n\")\n",
    "        print(event.result)\n",
    "        # Get transcription\n",
    "        transcription_handler = TranscriptionHandler()\n",
    "        feedback = await transcription_handler.get_transcription()\n",
    "        if not feedback:\n",
    "            feedback = input(event.prefix)\n",
    "        handler.ctx.send_event(\n",
    "            HumanResponseEvent(\n",
    "                response=feedback\n",
    "            )\n",
    "        )\n",
    "\n",
    "response = await handler\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw Workflow Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./jobpilot-workflow.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML, DisplayHandle\n",
    "\n",
    "def extract_html_content(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            html_content = file.read()\n",
    "            html_content = f\"\"\" <div style=\"width: 100%; height: 800px; overflow: hidden;\"> {html_content} </div>\"\"\"\n",
    "            return html_content\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error reading file: {str(e)}\")\n",
    "    \n",
    "draw_all_possible_flows(JobPilotWorkflow, filename='./jobpilot-workflow.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       " <div style=\"width: 100%; height: 800px; overflow: hidden;\"> <html>\n",
       "    <head>\n",
       "        <meta charset=\"utf-8\">\n",
       "        \n",
       "            <script src=\"lib/bindings/utils.js\"></script>\n",
       "            <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css\" integrity=\"sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\" />\n",
       "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js\" integrity=\"sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\"></script>\n",
       "            \n",
       "        \n",
       "<center>\n",
       "<h1></h1>\n",
       "</center>\n",
       "\n",
       "<!-- <link rel=\"stylesheet\" href=\"../node_modules/vis/dist/vis.min.css\" type=\"text/css\" />\n",
       "<script type=\"text/javascript\" src=\"../node_modules/vis/dist/vis.js\"> </script>-->\n",
       "        <link\n",
       "          href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css\"\n",
       "          rel=\"stylesheet\"\n",
       "          integrity=\"sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6\"\n",
       "          crossorigin=\"anonymous\"\n",
       "        />\n",
       "        <script\n",
       "          src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js\"\n",
       "          integrity=\"sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf\"\n",
       "          crossorigin=\"anonymous\"\n",
       "        ></script>\n",
       "\n",
       "\n",
       "        <center>\n",
       "          <h1></h1>\n",
       "        </center>\n",
       "        <style type=\"text/css\">\n",
       "\n",
       "             #mynetwork {\n",
       "                 width: 100%;\n",
       "                 height: 750px;\n",
       "                 background-color: #ffffff;\n",
       "                 border: 1px solid lightgray;\n",
       "                 position: relative;\n",
       "                 float: left;\n",
       "             }\n",
       "\n",
       "             \n",
       "\n",
       "             \n",
       "\n",
       "             \n",
       "        </style>\n",
       "    </head>\n",
       "\n",
       "\n",
       "    <body>\n",
       "        <div class=\"card\" style=\"width: 100%\">\n",
       "            \n",
       "            \n",
       "            <div id=\"mynetwork\" class=\"card-body\"></div>\n",
       "        </div>\n",
       "\n",
       "        \n",
       "        \n",
       "\n",
       "        <script type=\"text/javascript\">\n",
       "\n",
       "              // initialize global variables.\n",
       "              var edges;\n",
       "              var nodes;\n",
       "              var allNodes;\n",
       "              var allEdges;\n",
       "              var nodeColors;\n",
       "              var originalNodes;\n",
       "              var network;\n",
       "              var container;\n",
       "              var options, data;\n",
       "              var filter = {\n",
       "                  item : '',\n",
       "                  property : '',\n",
       "                  value : []\n",
       "              };\n",
       "\n",
       "              \n",
       "\n",
       "              \n",
       "\n",
       "              // This method is responsible for drawing the graph, returns the drawn network\n",
       "              function drawGraph() {\n",
       "                  var container = document.getElementById('mynetwork');\n",
       "\n",
       "                  \n",
       "\n",
       "                  // parsing and collecting nodes and edges from the python\n",
       "                  nodes = new vis.DataSet([{\"color\": \"#ADD8E6\", \"id\": \"_done\", \"label\": \"_done\", \"shape\": \"box\"}, {\"color\": \"#90EE90\", \"id\": \"StopEvent\", \"label\": \"StopEvent\", \"shape\": \"ellipse\"}, {\"color\": \"#ADD8E6\", \"id\": \"ask_question\", \"label\": \"ask_question\", \"shape\": \"box\"}, {\"color\": \"#90EE90\", \"id\": \"QueryEvent\", \"label\": \"QueryEvent\", \"shape\": \"ellipse\"}, {\"color\": \"#90EE90\", \"id\": \"ResponseEvent\", \"label\": \"ResponseEvent\", \"shape\": \"ellipse\"}, {\"color\": \"#ADD8E6\", \"id\": \"fill_application\", \"label\": \"fill_application\", \"shape\": \"box\"}, {\"color\": \"#90EE90\", \"id\": \"InputRequiredEvent\", \"label\": \"InputRequiredEvent\", \"shape\": \"ellipse\"}, {\"color\": \"#BEDAE4\", \"id\": \"external_step\", \"label\": \"external_step\", \"shape\": \"box\"}, {\"color\": \"#ADD8E6\", \"id\": \"generate_questions\", \"label\": \"generate_questions\", \"shape\": \"box\"}, {\"color\": \"#90EE90\", \"id\": \"GenerateQuestionEvent\", \"label\": \"GenerateQuestionEvent\", \"shape\": \"ellipse\"}, {\"color\": \"#90EE90\", \"id\": \"FeedbackEvent\", \"label\": \"FeedbackEvent\", \"shape\": \"ellipse\"}, {\"color\": \"#ADD8E6\", \"id\": \"get_feedback\", \"label\": \"get_feedback\", \"shape\": \"box\"}, {\"color\": \"#90EE90\", \"id\": \"HumanResponseEvent\", \"label\": \"HumanResponseEvent\", \"shape\": \"ellipse\"}, {\"color\": \"#ADD8E6\", \"id\": \"parse_form\", \"label\": \"parse_form\", \"shape\": \"box\"}, {\"color\": \"#90EE90\", \"id\": \"ParseFormEvent\", \"label\": \"ParseFormEvent\", \"shape\": \"ellipse\"}, {\"color\": \"#ADD8E6\", \"id\": \"setup\", \"label\": \"setup\", \"shape\": \"box\"}, {\"color\": \"#E27AFF\", \"id\": \"StartEvent\", \"label\": \"StartEvent\", \"shape\": \"ellipse\"}]);\n",
       "                  edges = new vis.DataSet([{\"arrows\": \"to\", \"from\": \"StopEvent\", \"to\": \"_done\"}, {\"arrows\": \"to\", \"from\": \"ask_question\", \"to\": \"ResponseEvent\"}, {\"arrows\": \"to\", \"from\": \"QueryEvent\", \"to\": \"ask_question\"}, {\"arrows\": \"to\", \"from\": \"fill_application\", \"to\": \"InputRequiredEvent\"}, {\"arrows\": \"to\", \"from\": \"InputRequiredEvent\", \"to\": \"external_step\"}, {\"arrows\": \"to\", \"from\": \"fill_application\", \"to\": \"StopEvent\"}, {\"arrows\": \"to\", \"from\": \"ResponseEvent\", \"to\": \"fill_application\"}, {\"arrows\": \"to\", \"from\": \"generate_questions\", \"to\": \"QueryEvent\"}, {\"arrows\": \"to\", \"from\": \"GenerateQuestionEvent\", \"to\": \"generate_questions\"}, {\"arrows\": \"to\", \"from\": \"FeedbackEvent\", \"to\": \"generate_questions\"}, {\"arrows\": \"to\", \"from\": \"get_feedback\", \"to\": \"FeedbackEvent\"}, {\"arrows\": \"to\", \"from\": \"get_feedback\", \"to\": \"StopEvent\"}, {\"arrows\": \"to\", \"from\": \"HumanResponseEvent\", \"to\": \"get_feedback\"}, {\"arrows\": \"to\", \"from\": \"external_step\", \"to\": \"HumanResponseEvent\"}, {\"arrows\": \"to\", \"from\": \"parse_form\", \"to\": \"GenerateQuestionEvent\"}, {\"arrows\": \"to\", \"from\": \"ParseFormEvent\", \"to\": \"parse_form\"}, {\"arrows\": \"to\", \"from\": \"setup\", \"to\": \"ParseFormEvent\"}, {\"arrows\": \"to\", \"from\": \"StartEvent\", \"to\": \"setup\"}]);\n",
       "\n",
       "                  nodeColors = {};\n",
       "                  allNodes = nodes.get({ returnType: \"Object\" });\n",
       "                  for (nodeId in allNodes) {\n",
       "                    nodeColors[nodeId] = allNodes[nodeId].color;\n",
       "                  }\n",
       "                  allEdges = edges.get({ returnType: \"Object\" });\n",
       "                  // adding nodes and edges to the graph\n",
       "                  data = {nodes: nodes, edges: edges};\n",
       "\n",
       "                  var options = {\n",
       "    \"configure\": {\n",
       "        \"enabled\": false\n",
       "    },\n",
       "    \"edges\": {\n",
       "        \"color\": {\n",
       "            \"inherit\": true\n",
       "        },\n",
       "        \"smooth\": {\n",
       "            \"enabled\": true,\n",
       "            \"type\": \"dynamic\"\n",
       "        }\n",
       "    },\n",
       "    \"interaction\": {\n",
       "        \"dragNodes\": true,\n",
       "        \"hideEdgesOnDrag\": false,\n",
       "        \"hideNodesOnDrag\": false\n",
       "    },\n",
       "    \"physics\": {\n",
       "        \"enabled\": true,\n",
       "        \"stabilization\": {\n",
       "            \"enabled\": true,\n",
       "            \"fit\": true,\n",
       "            \"iterations\": 1000,\n",
       "            \"onlyDynamicEdges\": false,\n",
       "            \"updateInterval\": 50\n",
       "        }\n",
       "    }\n",
       "};\n",
       "\n",
       "                  \n",
       "\n",
       "\n",
       "                  \n",
       "\n",
       "                  network = new vis.Network(container, data, options);\n",
       "\n",
       "                  \n",
       "\n",
       "                  \n",
       "\n",
       "                  \n",
       "\n",
       "\n",
       "                  \n",
       "\n",
       "                  return network;\n",
       "\n",
       "              }\n",
       "              drawGraph();\n",
       "        </script>\n",
       "    </body>\n",
       "</html> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "isolated": true
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "html_content = extract_html_content('./jobpilot-workflow.html')\n",
    "display(HTML(html_content), metadata=dict(isolated=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
